# AutoJudge â€“ Programming Problem Difficulty Prediction

## ğŸ“Œ Project Overview
**AutoJudge** is a machine learning system that automatically predicts the difficulty of
programming problems using only their textual descriptions.

The system performs **two tasks**:
1. **Classification** â€“ Predicts difficulty class (*Easy / Medium / Hard*)
2. **Regression** â€“ Predicts a numerical difficulty score on a **1â€“10 scale**

The project uses **Natural Language Processing (NLP)** with classical machine learning
models and provides a **Streamlit-based web interface** for real-time predictions.

---

## ğŸ“‚ Dataset
- **Source:** Competitive programming problem statements
- **Format:** JSONL
- **Fields:**
  - Title
  - Problem Description
  - Input Description
  - Output Description
  - Difficulty Class (Easy / Medium / Hard)
  - Difficulty Score (1â€“10)

The dataset contains **no missing values**, allowing direct preprocessing.

---

## ğŸ› ï¸ Methodology

### ğŸ”¹ Data Preprocessing
- Combined all text fields into a single `full_text`
- Text cleaning:
  - Lowercasing
  - Removal of extra spaces and line breaks
- Same preprocessing pipeline used for both classification and regression

---

### ğŸ”¹ Feature Engineering

#### Text Features
- **TF-IDF Vectorization**
  - Unigrams and bigrams
  - High-dimensional sparse representation

#### Keyword-Based Difficulty Signals
To capture explicit difficulty-related cues present in problem statements, 
keyword-based features were engineered using domain knowledge:

- **Easy keywords:** array, loop, print, sum, basic, integer, simple  
- **Medium keywords:** binary search, DFS, BFS, sorting, greedy, stack, queue  
- **Hard keywords:** DP, dynamic programming, bitmask, segment tree, union find, flow, graph theory  

For each problem, the **frequency of Easy, Medium, and Hard keywords** was computed.
These features help the model capture **semantic hints of problem complexity** that
may not be fully represented by TF-IDF alone.

> During experimentation, it was observed that excessive reliance on explicit difficulty
keywords could introduce bias. Therefore, keyword features were carefully evaluated and
used only when they improved generalization without causing label leakage.

---

#### Numeric Features
- Text length
- Word count
- Maximum numeric constraint extracted from text

Numeric features are **standardized** and concatenated with TF-IDF features.

---

### ğŸ”¹ Feature Selection
To handle high-dimensional text data:
- **SelectKBest (Chi-square)** for classification
- **SelectKBest (F-regression)** for regression

Feature selection is applied **only on training data**, preventing data leakage and
improving generalization.

---

## ğŸ¤– Models Used

### ğŸ”¸ Classification
- Logistic Regression (baseline)
- **Random Forest Classifier**
- Support Vector Machine (SVM)

**Why RF?**
- Performs well on sparse, high-dimensional TF-IDF features
- Effectively separates Easy, Medium, and Hard classes
- Robust to class imbalance

### ğŸ”¸ Regression
- Linear Regression
- Ridge Regression
- Random Forest Regression
- Gradient Boosting Regression
- **XGBoost Regression** âœ… *(Final model)*

Regression models were trained **directly on the original 1â€“10 scale** for better
interpretability.

---

## ğŸ“Š Results

### ğŸ”¹ Classification Results
- **Accuracy:** ~55.16%
- **Best Model:** RF + SelectKBest + Optuna
- Feature selection and keyword-aware features improved macro F1-score

---

### ğŸ”¹ Regression Results
- **MAE:** ~1.6549
- **RMSE:** ~1.9969
- **RÂ²:** ~0.1783
- **Best Model:** XGBoost Regression

The regression model predicts difficulty scores with an average error of
**less than one difficulty point**, which is strong given the subjective nature
of difficulty estimation.

---

## ğŸŒ Web Application
A lightweight web application was built using **Streamlit**.

### Inputs
- Problem description
- Input description
- Output description

### Outputs
- Predicted difficulty class (Easy / Medium / Hard)
- Predicted difficulty score (1â€“10 scale)

The web app uses the **same trained models and preprocessing pipeline**
as the final notebook.

---

## How to Run the Project Locally

### Step 1: Clone the repository
```bash
git clone https://github.com/Vishalmeena0658/AutoJudge.git
cd AutoJudge

### Step 2: Install dependencies
pip install -r requirements.txt

###Step 3: Run the web app
python -m streamlit run app.py

##Project Folder Structure

AutoJudge/
â”‚
â”œâ”€â”€ app.py                      # Streamlit web application
â”œâ”€â”€ AutoJudge_final.ipynb       # Final training & evaluation notebook
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ problems_data.jsonl     # Dataset
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl
â”‚   â”œâ”€â”€ numeric_scaler.pkl
â”‚   â”œâ”€â”€ selectkbest.pkl
â”‚   â”œâ”€â”€ reg_selector.pkl
â”‚   â”œâ”€â”€ rf_final_model.pkl
â”‚   â”œâ”€â”€ xgb_regressor.pkl
â”‚   â””â”€â”€ label_encoder.pkl
â”‚
â””â”€â”€ Report.pdf    # Final project report

